[{"content":"Shell 参数处理特殊字符 #!/bin/bash echo \u0026#34;Starting program at $(date)\u0026#34; # 1. $( CMD ) 运行 CMD 命令 echo \u0026#34;Running program $0 with $# arguments with pid $$\u0026#34; # 2. 见特殊字符表 for file in \u0026#34;$@\u0026#34;; do # 3. 见特殊字符表 grep foobar \u0026#34;$file\u0026#34; \u0026gt; /dev/null 2\u0026gt; /dev/null # 4. \u0026#34;2\u0026gt; /dev/null\u0026#34; 把标准错误重定向到/dev/null if [[ $? -ne 0 ]]; then # 5. 见特殊字符表；在比较操作中使用双中括号 echo \u0026#34;File $file does not have any foobar, adding one\u0026#34; echo \u0026#34;# foobar\u0026#34; \u0026gt;\u0026gt; \u0026#34;$file\u0026#34; fi done 特殊字符表 参数处理 说明 $0 Shell Script 名字 $# 参数个数 $$ 进程 PID $@ 所有参数 $? 上一条命令的返回码 test 命令的双中括号 Differences Between Single and Double Brackets in Bash.Differences Between Single and Double Brackets in Bash.\nsave-bgsave 脚本整理 #!/bin/bash function help() # 1. 函数声明 { echo \u0026#34;usage: safe-bgsave -t THRESHOLD -i INTERVAL -p PORT.\u0026#34; echo \u0026#34; safe-bgsave -t 20 -i 1 -p 7002\u0026#34; exit } # success return 0; fail return 1. function safe_bgsave() { # check security local available=`free -g | grep Mem | awk \u0026#39;{print $7}\u0026#39;` # 2. awk 选取第 7 列 if [ ${available} -le ${THRESHOLD} ] then echo `date` \u0026#34; Available memory: ${available} \u0026lt;= threshold: ${THRESHOLD}. Do not process bgsave.\u0026#34; \u0026gt;\u0026gt; ${LOG_PATH} 2\u0026gt;\u0026amp;1 return 1 fi # bgsave local bgsave=`${REDIS_CLI} -p ${PORT} bgsave` echo `date` \u0026#34; ${bgsave}.\u0026#34; \u0026gt;\u0026gt; ${LOG_PATH} 2\u0026gt;\u0026amp;1 # 3. 字符串拼接；\u0026#34;2\u0026gt;\u0026amp;1\u0026#34; 将标准错误重定向到标准输出 sleep ${INTERVAL} local pid=`ps -ef | grep \u0026#34;${PROCESS_NAME}\u0026#34; | grep -v grep | awk \u0026#39;{print $2}\u0026#39;` while [ -n \u0026#34;${pid}\u0026#34; ] do available=`free -g | grep Mem | awk \u0026#39;{print $7}\u0026#39;` if [ ${available} -le ${THRESHOLD} ] then echo `date` \u0026#34; bgsave failed. Available memory: ${available} \u0026lt;= threshold: ${THRESHOLD}. Trying to kill process.\u0026#34; \u0026gt;\u0026gt; ${LOG_PATH} 2\u0026gt;\u0026amp;1 kill \u0026#34;${pid}\u0026#34; return 1 fi sleep ${INTERVAL} pid=`ps -ef | grep \u0026#34;${PROCESS_NAME}\u0026#34; | grep -v grep | awk \u0026#39;{print $2}\u0026#39;` done echo `date` \u0026#34; bgsave success.\u0026#34; \u0026gt;\u0026gt; ${LOG_PATH} 2\u0026gt;\u0026amp;1 return 0 } # Begin [ $# -ne 6 ] \u0026amp;\u0026amp; help # 4. 检查参数个数；调用 help 函数 while [ -n \u0026#34;$1\u0026#34; ] # 5. 参数赋值的写法 do case \u0026#34;$1\u0026#34; in -t) THRESHOLD=$2 shift 2 # 6. 将参数数组向左移动两位 ;; -i) INTERVAL=$2 shift 2 ;; -p) PORT=$2 shift 2 ;; *) help ;; esac done REDIS_CLI=\u0026#34;redis-cli\u0026#34; PROCESS_NAME=\u0026#34;redis-rdb-bgsave\u0026#34; LOG_PATH=\u0026#34;/opt/data/redis/safe_bgsave.${PORT}.log\u0026#34; safe_bgsave if [ $? -eq 1 ] then exit 1 fi exit 0 ","permalink":"https://fullzsy.github.io/posts/tech/shell-script-%E6%95%B4%E7%90%86/","summary":"整理 Shell Script 语法","title":"Shell Script 整理"},{"content":"算法分析的方法 Observe some feature of the natural world, generally with precise measurements. Hypothesize a model that is consistent with the observations. Predict events using the hypothesis. Verify the predictions by making further observations. Validate by repeating until the hypothesis and observations agree. 算法运行时间实验 程序执行速度的快慢通常取决于问题的规模。\n通过观察代码初步预测程序的执行时间。\n使用类似 DoublingTest 方法不断增加问题规模并计时，进入“预测——验证”循环。\npublic class DoublingTest { public static double timeTrial(int N) { // Time ThreeSum.count() for N random 6-digit ints. int MAX = 1000000; int[] a = new int[N]; for (int i = 0; i \u0026lt; N; i++) a[i] = StdRandom.uniformInt(-MAX, MAX); Stopwatch timer = new Stopwatch(); int cnt = ThreeSum.count(a); return timer.elapsedTime(); } public static void main(String[] args) { // Print table of running times. for (int N = 250; true; N += N) { // Print time for problem size N. double time = timeTrial(N); StdOut.printf(\u0026#34;%7d %5.1f\\n\u0026#34;, N, time); } } } 分析实验数据，得到其运行时间的数学模型。\n运行时间的数学模型 得到运行时间的数学模型步骤：（书：P114）\n确定输入模型，定义问题规模 识别内循环 根据内循环中的操作确定成本模型 对于给定的输入，判断这些操作的执行频率 增长数量级 描述 增长的数量级 典型代码 说明 举例 常数级别 1 a = b + c 普通语句 两个数相加 对数级别 logN 二分查找 二分策略 二分查找 线性级别 N 循环 循环 循环查找最大元素 线性对数级别 NlogN 归并排序 Merge.sort 和 快速排序 Quick.sort 归并排序，快速排序 归并排序，快速排序 平方级别 N^2 选择排序 Selection.sort 和 插入排序 Insertion.sort 双层循环 选择排序，插入排序 立方级别 N^3 ThreeSum 三层循环 三层循环 三层循环 指数级别 2^N 第六章 穷举查找 检查所有子集 倍率实验 通过倍率实验能够简单有效地预测任意程序的性能并判断它们运行时间大致的增长数量级，但对比值没有极限的算法无效。\npublic class DoublingRatio { public static double timeTrial(int N) { // Time ThreeSum.count() for N random 6-digit ints. int MAX = 1000000; int[] a = new int[N]; for (int i = 0; i \u0026lt; N; i++) a[i] = StdRandom.uniformInt(-MAX, MAX); Stopwatch timer = new Stopwatch(); int cnt = ThreeSum.count(a); return timer.elapsedTime(); } public static void main(String[] args) { double prev = timeTrial(125); for (int N = 250; true; N += N) { double time = timeTrial(N); StdOut.printf(\u0026#34;%6d %7.1f \u0026#34;, N, time); StdOut.printf(\u0026#34;%5.1f\\n\u0026#34;, time / prev); prev = time; } } } 在有性能问题的情况家应该考虑对编写过的所有程序进行倍率实验，以便能找到性能问题。\n","permalink":"https://fullzsy.github.io/posts/read/%E7%AE%97%E6%B3%95%E7%AC%AC%E5%9B%9B%E7%89%88_04_%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/","summary":"《算法第四版》算法分析","title":"《算法第四版》算法分析"},{"content":"背包 背包 API：\n背包 public class Bag\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; Bag() 创建一个背包 void add(Item item) 添加一个元素 boolean isEmpty() 背包是否为空 int size() 背包中元素数量 背包是一种不支持从中删除元素的集合数据类型——他的目的是帮助用例收集元素并迭代遍历所有收集到的元素。\n背包的链表实现 import java.util.Iterator; public class Bag\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; { private Node first; private class Node { Item item; Node next; } public void add(Item item) { Node oldFirst = first; first = new Node(); first.item = item; first.next = oldFirst; } @Override public Iterator\u0026lt;Item\u0026gt; iterator() { return new ListIterator(); } private class ListIterator implements Iterator\u0026lt;Item\u0026gt; { private Node current = first; @Override public boolean hasNext() { return current != null; } @Override public Item next() { Item item = current.item; current = current.next; return item; } } } 先进先出（FIFO）队列 队列 API：\n先进先出（FIFO）队列 public class Queue\u0026lt;Item\u0026gt; implements Interable\u0026lt;Item\u0026gt; Queue() 创建空队列 void enqueue(Item item) 添加一个元素 Item dequeue() 删除最早添加的元素 boolean isEmpty() 队列是否为空 int size() 队列中的元素数量 先进先出队列是一种基于先进先出（FIFO）策略的集合类型。\n队列的实现 队列的链表实现 import java.util.Iterator; public class Queue\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; { private Node first; private Node last; private int size; private class Node { Item item; Node next; } public boolean isEmpty() { return first == null; } public int size() { return size; } public void enqueue(Item item) { Node oldLast = last; last = new Node(); last.item = item; last.next = null; if (isEmpty()) { first = last; } else { oldLast.next = last; } size++; } public Item dequeue() { Item item = first.item; first = first.next; if (isEmpty()) { last = null; } size--; return item; } @Override public Iterator\u0026lt;Item\u0026gt; iterator() { return new ListIterator(); } private class ListIterator implements Iterator\u0026lt;Item\u0026gt; { private Node current = first; @Override public boolean hasNext() { return current != null; } @Override public Item next() { Item item = current.item; current = current.next; return item; } } } 后进先出（LIFO）栈 栈 API：\npublic class Stack\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; Stack() 创建一个空栈 void push() 添加一个元素 Item pop() 删除最近添加的元素 boolean isEmpty() 栈是否为空 int size() 栈中元素数量 栈的实现 栈的数组实现 import java.util.Iterator; public class ResizingArrayStack\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; { private Item[] array = (Item[]) new Object[1]; private int size = 0; public boolean isEmpty() { return size == 0; } public int size() { return size; } public void push(Item item) { if (size == array.length) { resize(2 * array.length); } array[size++] = item; } public Item pop() { Item item = array[--size]; array[size] = null; if (size \u0026gt; 0 \u0026amp;\u0026amp; size == array.length / 4) { resize(array.length / 2); } return item; } private void resize(int length) { Item[] temp = (Item[]) new Object[length]; for (int i = 0; i \u0026lt; size; i++) { temp[i] = array[i]; } array = temp; } @Override public Iterator\u0026lt;Item\u0026gt; iterator() { return new ReverseArrayIterator(); } private class ReverseArrayIterator implements Iterator\u0026lt;Item\u0026gt; { private int index = size; @Override public boolean hasNext() { return index \u0026gt; 0; } @Override public Item next() { return array[--index]; } } } 栈的链表实现 import java.util.Iterator; public class Stack\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; { private Node first; private int size; private class Node { Item item; Node next; } public boolean isEmpty() { return first == null; } public int size() { return size; } public void push(Item item) { Node oldFirst = first; first = new Node(); first.item = item; first.next = oldFirst; size++; } public Item pop() { Item item = first.item; first = first.next; size--; return item; } @Override public Iterator\u0026lt;Item\u0026gt; iterator() { return new ListIterator(); } private class ListIterator implements Iterator\u0026lt;Item\u0026gt; { private Node current = first; @Override public boolean hasNext() { return current != null; } @Override public Item next() { Item item = current.item; current = current.next; return item; } } } 栈的应用 Dijkstra 双栈算术表达式求值算法\npublic class Evaluate { public static void main(String[] args) { Stack\u0026lt;String\u0026gt; ops = new Stack\u0026lt;\u0026gt;(); Stack\u0026lt;Double\u0026gt; vals = new Stack\u0026lt;\u0026gt;(); while (!StdIn.isEmpty()) { String s = StdIn.readString(); switch (s) { case \u0026#34;(\u0026#34; -\u0026gt; { ; } case \u0026#34;+\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;*\u0026#34;, \u0026#34;/\u0026#34;, \u0026#34;sqrt\u0026#34; -\u0026gt; ops.push(s); case \u0026#34;)\u0026#34; -\u0026gt; { String op = ops.pop(); double v = vals.pop(); v = switch (op) { case \u0026#34;+\u0026#34; -\u0026gt; vals.pop() + v; case \u0026#34;-\u0026#34; -\u0026gt; vals.pop() - v; case \u0026#34;*\u0026#34; -\u0026gt; vals.pop() * v; case \u0026#34;/\u0026#34; -\u0026gt; vals.pop() / v; case \u0026#34;sqrt\u0026#34; -\u0026gt; Math.sqrt(v); default -\u0026gt; v; }; vals.push(v); } default -\u0026gt; vals.push(Double.parseDouble(s)); } } StdOut.println(vals.pop()); } } ","permalink":"https://fullzsy.github.io/posts/read/%E7%AE%97%E6%B3%95%E7%AC%AC%E5%9B%9B%E7%89%88_03_%E8%83%8C%E5%8C%85%E9%98%9F%E5%88%97%E5%92%8C%E6%A0%88/","summary":"《算法第四版》背包、队列和栈","title":"《算法第四版》背包、队列和栈"},{"content":"一个线程不安全的现象 一个数组实现的环形缓冲区，变量 readPos 和 writePos 分别记录下一个读取的索引和下一个写入的索引。当缓冲区为空时，消费者会在数据存入缓冲区前等待。当缓冲区满时，生产者会等待将数据存入缓冲区。\npublic class RingBuffer\u0026lt;Item\u0026gt; { private final Item[] buffer; private int readPos; private int writePos; RingBuffer(int capacity) { this.buffer = (Item[]) new Object[capacity]; this.readPos = 0; this.writePos = 0; } public void write(Item item) { while (isFull()) ; buffer[writePos] = item; writePos = (writePos + 1) % buffer.length; } public Item read() { while (isEmpty()) ; Item item = buffer[readPos]; readPos = (readPos + 1) % buffer.length; return item; } private boolean isEmpty() { return readPos == writePos; } private boolean isFull() { return ((writePos + 1) % buffer.length) == readPos; } public static void main(String[] args) { RingBuffer\u0026lt;Integer\u0026gt; ringBuffer = new RingBuffer\u0026lt;\u0026gt;(10); Thread writer1 = new Thread(() -\u0026gt; { for (int item = 0; item \u0026lt; Integer.MAX_VALUE; item++) { ringBuffer.write(item); } }); writer1.start(); while (true) { StdOut.println(ringBuffer.read()); } } } 在运行此测试用例时发现两个线程都容易进入死循环。写入线程一直认为缓冲区是满的，消费线程一直认为缓冲区是空的。经过排查，此现象是 readPos 和 writePos 变量不一致导致的。\n在写入线程中，writePos 变量只会被写入线程修改，因此该变量对于写入线程来说始终是最新值。而写入线程调用 isFull 方法的 readPos 变量会被读取线程修改，导致写入线程中 readPos 变量是旧数据。\n在读取线程中，readPos 变量只会被读取线程修改，因此该变量对于读取线程来说始终是最新值。而读取线程调用 isEmpty 方法的 writePos 变量会被写入线程修改，导致读取线程中 writePos 变量是旧数据。\n解决方案 将 readPos 和 writePos 改为 volatile 变量，在这个场景中能够保证这两个变量的线程安全。\n那么 volatile 变量在此场景中是如何保证线程安全的呢？\nvolatile 变量机制 可见行保证 对于非 volatile 变量，JVM 不会保证线程修改变量会被立即从 CPU 缓存中回写到主内存中。使得另一个线程可能会从主内存读取到该变量的旧值。\n对于 volatile 变量，JVM 会保证线程每次都会从主内存中读取该变量。并且对该变量的修改会被立即回写到主内存。此时其余所有线程都会看到该变量的最新值。\nhappens-before 保证 happens-before 保证会对指令重排序进行限制。\n对 volatile 变量进行写入操作之前的所有指令不会因指令重排序导致这些指令在写入操作的后面； 对 volatile 变量进行读取操作之后的所有指令不会因指令重排序导致这些指令在写入操作的之前。 即本应在 volatile 变量读取与写入操作之间的指令，不会因为指令重排序导致这些指令在变量读取与写入操作之外。\nvolatile 变量何时是线程安全的？ 在以下两个场景，volatile 变量是线程安全的：\n当只有一个线程向 volatile 变量写入，其余多个线程仅读取该变量时，总会读取最新的数据，此时是线程安全的； 当多个线程向 volatile 变量写入并且对变量的操作是原子操作（被写入的新值不依赖旧值），此时是线程安全的。 一个 volitile 变量例子 public class Singleton { private volatile static Singleton singleton; private Singleton (){} public static Singleton getSingleton() { if (singleton == null) { synchronized (Singleton.class) { if (singleton == null) { singleton = new Singleton(); } } } return singleton; } } 参考资料 Concurrency in Java: \u0026ldquo;synchronized\u0026rdquo; and \u0026ldquo;volatile\u0026rdquo; keywords\nVolatile Variables and Thread Safety\n","permalink":"https://fullzsy.github.io/posts/tech/volatile%E5%8F%98%E9%87%8F%E4%B8%8E%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/","summary":"Volatile变量与线程安全","title":"Volatile变量与线程安全"},{"content":"现版本存在的问题 目前在生产环境中部署的旧版本Redis存在的问题：\n大 key 过期删除引发集群节点阻塞失去响应不可用； 内存碎片高，内存使用率低； 运维时，手动主从切换总会产生复制风暴问题，主从无法同步； bgsave 内存消耗较高，有 OOM 风险。 推进版本升级 由于以上原因，开始推进 Redis 版本升级。\n版本升级工作流程：\n通过文档与源码，调研 Redis 新特性； 客户端兼容性改造； 功能测试、性能测试和稳定性测试； 开发配套监控和运维工具； 推进上线。 ","permalink":"https://fullzsy.github.io/posts/tech/redis%E5%8D%87%E7%BA%A7_0_%E5%BC%80%E7%AF%87/","summary":"记录Redis升级流程。","title":"Redis升级——开篇"},{"content":"自 Redis 4 版本引入了异步删除方法 unlink，官方对该接口的解释：\n“This command is very similar to DEL: it removes the specified keys. Just like DEL a key is ignored if it does not exist. However the command performs the actual memory reclaiming in a different thread, so it is not blocking, while DEL is. This is where the command name comes from: the command just unlinks the keys from the keyspace. The actual removal will happen later asynchronously.”\n可知 unlink 与 del 用法相同，只不过内存回收在另一个不同线程中，内存回收操作在 unlink 方法调用结束之后，因此是非阻塞方法。\n源码分析 异步删除流程 同步删除与异步删除的方法入口分别为 delCommand 方法与 unlinkCommand 方法。\nvoid delCommand(client *c) { delGenericCommand(c,server.lazyfree_lazy_user_del); } void unlinkCommand(client *c) { delGenericCommand(c,1); } 这两个方法都调用 delGenericCommand 方法，server.lazyfree_lazy_user_del 可通过配置文件配置，配置后可以使 del 命令与 unlink 命令完全相同。\n/* This command implements DEL and LAZYDEL. */ void delGenericCommand(client *c, int lazy) { int numdel = 0, j; for (j = 1; j \u0026lt; c-\u0026gt;argc; j++) { expireIfNeeded(c-\u0026gt;db,c-\u0026gt;argv[j],0); // 判断传入的 lazy 值选择异步删除或同步删除 int deleted = lazy ? dbAsyncDelete(c-\u0026gt;db,c-\u0026gt;argv[j]) : dbSyncDelete(c-\u0026gt;db,c-\u0026gt;argv[j]); if (deleted) { signalModifiedKey(c,c-\u0026gt;db,c-\u0026gt;argv[j]); notifyKeyspaceEvent(NOTIFY_GENERIC, \u0026#34;del\u0026#34;,c-\u0026gt;argv[j],c-\u0026gt;db-\u0026gt;id); server.dirty++; numdel++; } } addReplyLongLong(c,numdel); } delGenericCommand 方法判断传入的 lazy 参数值决定异步删除或者同步删除。\n/* Delete a key, value, and associated expiration entry if any, from the DB */ int dbSyncDelete(redisDb *db, robj *key) { return dbGenericDelete(db, key, 0); } /* Delete a key, value, and associated expiration entry if any, from the DB. If * the value consists of many allocations, it may be freed asynchronously. */ int dbAsyncDelete(redisDb *db, robj *key) { return dbGenericDelete(db, key, 1); } 同步删除和异步删除都是调用 dbGenericDelete 方法，仅传入的 async 参数不同。\n/* Helper for sync and async delete. */ static int dbGenericDelete(redisDb *db, robj *key, int async) { /* Deleting an entry from the expires dict will not free the sds of * the key, because it is shared with the main dictionary. */ // 删除 expires 字典中该 key，但不会删除 SDS 结构，因为该 SDS 在 dict 字典中被共享。 if (dictSize(db-\u0026gt;expires) \u0026gt; 0) dictDelete(db-\u0026gt;expires,key-\u0026gt;ptr); // 数据库字典中移除 key，不释放内存。 dictEntry *de = dictUnlink(db-\u0026gt;dict,key-\u0026gt;ptr); if (de) { robj *val = dictGetVal(de); /* Tells the module that the key has been unlinked from the database. */ moduleNotifyKeyUnlink(key,val,db-\u0026gt;id); /* We want to try to unblock any client using a blocking XREADGROUP */ if (val-\u0026gt;type == OBJ_STREAM) signalKeyAsReady(db,key,val-\u0026gt;type); if (async) { // 异步释放内存 freeObjAsync(key, val, db-\u0026gt;id); dictSetVal(db-\u0026gt;dict, de, NULL); } if (server.cluster_enabled) slotToKeyDelEntry(de, db); // 释放内存 dictFreeUnlinkedEntry(db-\u0026gt;dict,de); return 1; } else { return 0; } } /* You need to call this function to really free the entry after a call * to dictUnlink(). It\u0026#39;s safe to call this function with \u0026#39;he\u0026#39; = NULL. */ void dictFreeUnlinkedEntry(dict *d, dictEntry *he) { if (he == NULL) return; dictFreeKey(d, he); dictFreeVal(d, he); zfree(he); } dbGenericDelete 方法首先将 key 在 expires 字典中删除并释放内存，再在 dict 字典中移除该 key，但此时不释放内存。 通过 async 参数判断是否需要异步释放内存，若需要则会调用 freeObjAsync 方法进行异步释放内存，若不需要异步释放内存，则在 dictFreeUnlinkedEntry 方法中直接释放。 若进入 freeObjAsync 方法但不满足异步释放条件（在 freeObjAsync 方法中），也会在 dictFreeUnlinkedEntry 方法中直接释放。\n/* If there are enough allocations to free the value object asynchronously, it * may be put into a lazy free list instead of being freed synchronously. The * lazy free list will be reclaimed in a different bio.c thread. If the value is * composed of a few allocations, to free in a lazy way is actually just * slower... So under a certain limit we just free the object synchronously. */ #define LAZYFREE_THRESHOLD 64 /* Free an object, if the object is huge enough, free it in async way. */ void freeObjAsync(robj *key, robj *obj, int dbid) { // 计算异步删除阈值 size_t free_effort = lazyfreeGetFreeEffort(key,obj,dbid); /* Note that if the object is shared, to reclaim it now it is not * possible. This rarely happens, however sometimes the implementation * of parts of the Redis core may call incrRefCount() to protect * objects, and then call dbDelete(). */ if (free_effort \u0026gt; LAZYFREE_THRESHOLD \u0026amp;\u0026amp; obj-\u0026gt;refcount == 1) { atomicIncr(lazyfree_objects,1); // 任务超过异步删除阈值，创建异步删除任务 bioCreateLazyFreeJob(lazyfreeFreeObject,1,obj); } else { decrRefCount(obj); } } 重点看 freeObjAsync 方法，先计算该 key 的异步删除阈值，若大于阈值 64，则为该 key 创建异步删除任务。\nvoid bioCreateLazyFreeJob(lazy_free_fn free_fn, int arg_count, ...) { va_list valist; /* Allocate memory for the job structure and all required * arguments */ bio_job *job = zmalloc(sizeof(*job) + sizeof(void *) * (arg_count)); job-\u0026gt;free_args.free_fn = free_fn; va_start(valist, arg_count); for (int i = 0; i \u0026lt; arg_count; i++) { job-\u0026gt;free_args.free_args[i] = va_arg(valist, void *); } va_end(valist); // 提交任务 bioSubmitJob(BIO_LAZY_FREE, job); } void bioSubmitJob(int type, bio_job *job) { // 互斥锁 pthread_mutex_lock(\u0026amp;bio_mutex[type]); // 添加任务至末尾 listAddNodeTail(bio_jobs[type],job); bio_pending[type]++; // 发送信号唤醒阻塞线程 pthread_cond_signal(\u0026amp;bio_newjob_cond[type]); pthread_mutex_unlock(\u0026amp;bio_mutex[type]); } bioCreateLazyFreeJob 方法创建任务并调用 bioSubmitJob 方法提交任务到 job 数据结构中。\nvoid *bioProcessBackgroundJobs(void *arg) { bio_job *job; unsigned long type = (unsigned long) arg; sigset_t sigset; /* Check that the type is within the right interval. */ if (type \u0026gt;= BIO_NUM_OPS) { serverLog(LL_WARNING, \u0026#34;Warning: bio thread started with wrong type %lu\u0026#34;,type); return NULL; } switch (type) { case BIO_CLOSE_FILE: redis_set_thread_title(\u0026#34;bio_close_file\u0026#34;); break; case BIO_AOF_FSYNC: redis_set_thread_title(\u0026#34;bio_aof_fsync\u0026#34;); break; case BIO_LAZY_FREE: redis_set_thread_title(\u0026#34;bio_lazy_free\u0026#34;); break; } redisSetCpuAffinity(server.bio_cpulist); makeThreadKillable(); pthread_mutex_lock(\u0026amp;bio_mutex[type]); /* Block SIGALRM so we are sure that only the main thread will * receive the watchdog signal. */ sigemptyset(\u0026amp;sigset); sigaddset(\u0026amp;sigset, SIGALRM); if (pthread_sigmask(SIG_BLOCK, \u0026amp;sigset, NULL)) serverLog(LL_WARNING, \u0026#34;Warning: can\u0026#39;t mask SIGALRM in bio.c thread: %s\u0026#34;, strerror(errno)); while(1) { listNode *ln; /* The loop always starts with the lock hold. */ if (listLength(bio_jobs[type]) == 0) { pthread_cond_wait(\u0026amp;bio_newjob_cond[type],\u0026amp;bio_mutex[type]); continue; } /* Pop the job from the queue. */ ln = listFirst(bio_jobs[type]); job = ln-\u0026gt;value; /* It is now possible to unlock the background system as we know have * a stand alone job structure to process.*/ pthread_mutex_unlock(\u0026amp;bio_mutex[type]); /* Process the job accordingly to its type. */ if (type == BIO_CLOSE_FILE) { if (job-\u0026gt;fd_args.need_fsync) { redis_fsync(job-\u0026gt;fd_args.fd); } close(job-\u0026gt;fd_args.fd); } else if (type == BIO_AOF_FSYNC) { /* The fd may be closed by main thread and reused for another * socket, pipe, or file. We just ignore these errno because * aof fsync did not really fail. */ if (redis_fsync(job-\u0026gt;fd_args.fd) == -1 \u0026amp;\u0026amp; errno != EBADF \u0026amp;\u0026amp; errno != EINVAL) { int last_status; atomicGet(server.aof_bio_fsync_status,last_status); atomicSet(server.aof_bio_fsync_status,C_ERR); atomicSet(server.aof_bio_fsync_errno,errno); if (last_status == C_OK) { serverLog(LL_WARNING, \u0026#34;Fail to fsync the AOF file: %s\u0026#34;,strerror(errno)); } } else { atomicSet(server.aof_bio_fsync_status,C_OK); } } else if (type == BIO_LAZY_FREE) { job-\u0026gt;free_args.free_fn(job-\u0026gt;free_args.free_args); } else { serverPanic(\u0026#34;Wrong job type in bioProcessBackgroundJobs().\u0026#34;); } zfree(job); /* Lock again before reiterating the loop, if there are no longer * jobs to process we\u0026#39;ll block again in pthread_cond_wait(). */ pthread_mutex_lock(\u0026amp;bio_mutex[type]); listDelNode(bio_jobs[type],ln); bio_pending[type]--; /* Unblock threads blocked on bioWaitStepOfType() if any. */ pthread_cond_broadcast(\u0026amp;bio_step_cond[type]); } } 执行后台任务方法 bioProcessBackgroundJobs，详细过程不在讨论的主题中。\n结论：异步删除能够解决主线程阻塞问题。\n惰性删除与异步删除 Redis 惰性删除策略是否采用异步删除策略？\n在惰性删除中，Redis 在操作 Key 时会先判断该 Key 是否过期，若过期则会删除该 Key。\n/* This function is called when we are going to perform some operation * in a given key, but such key may be already logically expired even if * it still exists in the database. The main way this function is called * is via lookupKey*() family of functions. * * The behavior of the function depends on the replication role of the * instance, because by default replicas do not delete expired keys. They * wait for DELs from the master for consistency matters. However even * replicas will try to have a coherent return value for the function, * so that read commands executed in the replica side will be able to * behave like if the key is expired even if still present (because the * master has yet to propagate the DEL). * * In masters as a side effect of finding a key which is expired, such * key will be evicted from the database. Also this may trigger the * propagation of a DEL/UNLINK command in AOF / replication stream. * * On replicas, this function does not delete expired keys by default, but * it still returns 1 if the key is logically expired. To force deletion * of logically expired keys even on replicas, use the EXPIRE_FORCE_DELETE_EXPIRED * flag. Note though that if the current client is executing * replicated commands from the master, keys are never considered expired. * * On the other hand, if you just want expiration check, but need to avoid * the actual key deletion and propagation of the deletion, use the * EXPIRE_AVOID_DELETE_EXPIRED flag. * * The return value of the function is 0 if the key is still valid, * otherwise the function returns 1 if the key is expired. */ int expireIfNeeded(redisDb *db, robj *key, int flags) { if (!keyIsExpired(db,key)) return 0; /* If we are running in the context of a replica, instead of * evicting the expired key from the database, we return ASAP: * the replica key expiration is controlled by the master that will * send us synthesized DEL operations for expired keys. The * exception is when write operations are performed on writable * replicas. * * Still we try to return the right information to the caller, * that is, 0 if we think the key should be still valid, 1 if * we think the key is expired at this time. * * When replicating commands from the master, keys are never considered * expired. */ if (server.masterhost != NULL) { if (server.current_client == server.master) return 0; if (!(flags \u0026amp; EXPIRE_FORCE_DELETE_EXPIRED)) return 1; } /* In some cases we\u0026#39;re explicitly instructed to return an indication of a * missing key without actually deleting it, even on masters. */ if (flags \u0026amp; EXPIRE_AVOID_DELETE_EXPIRED) return 1; /* If clients are paused, we keep the current dataset constant, * but return to the client what we believe is the right state. Typically, * at the end of the pause we will properly expire the key OR we will * have failed over and the new primary will send us the expire. */ if (checkClientPauseTimeoutAndReturnIfPaused()) return 1; /* Delete the key */ // 删除 key deleteExpiredKeyAndPropagate(db,key); return 1; } expireIfNeeded 方法会调用 deleteExpiredKeyAndPropagate 方法删除 key。\n删除 key 时会读取 server.lazyfree_lazy_expire 配置决定删除策略。server.lazyfree_lazy_expire 可在配置文件中配置，配置后惰性删除将采用异步删除策略。\n/* Delete the specified expired key and propagate expire. */ void deleteExpiredKeyAndPropagate(redisDb *db, robj *keyobj) { mstime_t expire_latency; latencyStartMonitor(expire_latency); if (server.lazyfree_lazy_expire) // 采用异步删除策略 dbAsyncDelete(db,keyobj); else dbSyncDelete(db,keyobj); latencyEndMonitor(expire_latency); latencyAddSampleIfNeeded(\u0026#34;expire-del\u0026#34;,expire_latency); notifyKeyspaceEvent(NOTIFY_EXPIRED,\u0026#34;expired\u0026#34;,keyobj,db-\u0026gt;id); signalModifiedKey(NULL, db, keyobj); propagateDeletion(db,keyobj,server.lazyfree_lazy_expire); server.stat_expiredkeys++; } 结论：Redis 惰性删除在配置后可采用异步删除策略。\n定时删除与异步删除 定时任务 serverCron 方法最终会调用 activeExpireCycleTryExpire 方法，该方法仍会调用 deleteExpiredKeyAndPropagate 方法。\n/* Helper function for the activeExpireCycle() function. * This function will try to expire the key that is stored in the hash table * entry \u0026#39;de\u0026#39; of the \u0026#39;expires\u0026#39; hash table of a Redis database. * * If the key is found to be expired, it is removed from the database and * 1 is returned. Otherwise no operation is performed and 0 is returned. * * When a key is expired, server.stat_expiredkeys is incremented. * * The parameter \u0026#39;now\u0026#39; is the current time in milliseconds as is passed * to the function to avoid too many gettimeofday() syscalls. */ int activeExpireCycleTryExpire(redisDb *db, dictEntry *de, long long now) { long long t = dictGetSignedIntegerVal(de); if (now \u0026gt; t) { sds key = dictGetKey(de); robj *keyobj = createStringObject(key,sdslen(key)); // 删除 key deleteExpiredKeyAndPropagate(db,keyobj); decrRefCount(keyobj); return 1; } else { return 0; } } 结论：Redis 定时删除在配置后可采用异步删除策略。\n结论 异步删除策略能够在删除大 Key 时避免主线程阻塞，惰性删除与定时删除在配置后均可采用异步删除策略，因此异步删除能够解决大 Key 过期引起的主线程阻塞问题。\n","permalink":"https://fullzsy.github.io/posts/tech/redis%E5%8D%87%E7%BA%A7_1_%E5%BC%82%E6%AD%A5%E5%88%A0%E9%99%A4/","summary":"通过分析 Redis 异步删除源码，判断异步删除能否解决大 Key 过期阻塞主线程的问题。","title":"Redis异步删除解决大Key过期阻塞问题可行性分析"},{"content":"前提条件 查找的数组是有序的。\n递归写法 public class BinarySearch { public static int rank(int key, int[] a) { return rank(key, a, 0, a.length - 1); } public static int rank(int key, int[] a, int lo, int hi) { if (lo \u0026gt; hi) return -1; int mid = lo + (hi - lo) / 2; if (key \u0026lt; a[mid]) return rank(key, a, lo, hi - 1); else if (key \u0026gt; a[mid]) return rank(key, a, lo + 1, hi); else return mid; } } 循环写法 public class BinarySearch { public static int rank(int key, int[] a) { int lo = 0; int hi = a.length - 1; while (lo \u0026lt; hi) { int mid = lo + (hi - lo) / 2; if (key \u0026lt; a[mid]) hi = lo - 1; else if (key \u0026gt; a[mid]) lo = hi + 1; else return mid; } return -1; } } 二分查找 key 的最小索引 public class BinarySearch { public static int rank(int key, int[] a) { return rank(key, a, 0, a.length - 1); } public static int rank(int[] array, int key, int lo, int hi) { while (lo \u0026lt;= hi) { int mid = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (key \u0026lt;= array[mid]) { hi = mid - 1; } else { lo = mid + 1; } } if (lo == array.length) return -1; return array[lo] == key ? lo : -1; } } 二分查找极小(大)值 public class BinarySearch { public static int partialMin(int[] array) { assert array != null \u0026amp;\u0026amp; array.length \u0026gt; 0; int lo = 0; int hi = array.length - 1; while (lo \u0026lt;= hi) { int mid = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (mid == 0 || mid == array.length - 1) break; if (array[mid] \u0026lt; array[mid - 1] \u0026amp;\u0026amp; array[mid] \u0026lt; array[mid + 1]) { return mid; } else if (array[mid - 1] \u0026lt;= array[mid + 1]) { hi = mid - 1; } else { lo = mid + 1; } } return -1; } } ","permalink":"https://fullzsy.github.io/posts/read/%E7%AE%97%E6%B3%95%E7%AC%AC%E5%9B%9B%E7%89%88_01_%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/","summary":"《算法第四版》二分查找","title":"《算法第四版》二分查找"},{"content":"自然语言描述 计算两个非负整数 p 和 q 的最大公约数：若 q 是 0，则最大公约数为 p。否则，将 p 除以 q 得到余数 r，p 和 q 的最大公约数即为 q 和 r 的最大公约数。\n递归写法 public class Euclid { public static int gcd(int p, int q) { if (q == 0) return p; int r = p % q; return gcd(q, r); } } 循环写法 public class Euclid { public static int gcd(int p, int q) { if (q == 0) return p; while (q != 0) { int r = p % q; p = q; q = r; } return p; } } ","permalink":"https://fullzsy.github.io/posts/read/%E7%AE%97%E6%B3%95%E7%AC%AC%E5%9B%9B%E7%89%88_02_%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E6%9C%80%E5%A4%A7%E5%85%AC%E5%9B%A0%E6%95%B0/","summary":"《算法第四版》欧几里得算法求最大公因数","title":"《算法第四版》欧几里得算法求最大公因数"},{"content":"本系列文章作用 2023 年计划将《算法第四版》认真阅读一遍，在博客中整理常用的算法，把书读薄，时常复习，提高编码水平。\n习题仓库：https://github.com/FullZSY/algs4\n加油！\n","permalink":"https://fullzsy.github.io/posts/read/%E7%AE%97%E6%B3%95%E7%AC%AC%E5%9B%9B%E7%89%88_00_%E9%98%85%E8%AF%BB%E5%BC%80%E7%AF%87/","summary":"《算法第四版》阅读总结","title":"《算法第四版》阅读"},{"content":"关于我 姓名: Shuyang 职业: 后端程序员，负责 Redis 相关工作 ","permalink":"https://fullzsy.github.io/about/","summary":"关于我 姓名: Shuyang 职业: 后端程序员，负责 Redis 相关工作 ","title":"🙋🏻‍♂️关于"}]